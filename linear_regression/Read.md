# Защита персональных данных клиентов

Вам нужно защитить данные клиентов страховой компании «Хоть потоп». Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обоснуйте корректность его работы.

Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.


Задача обучения:

$$
w = \arg\min_w MSE(Xw, y)
$$

Задачу обучения можно записать формулой:

$$
w = (X^T X)^{-1} X^T y
$$

Для предсказания нам потребуется формула:

$$
a = Xw+w_0
$$

Обозначения:

- $X$ — матрица признаков (нулевой столбец состоит из единиц)

- $y$ — вектор целевого признака

- $P$ — матрица, на которую умножаются признаки

- $w$ — вектор весов линейной регрессии (нулевой элемент равен сдвигу)

- $w_0$ - сдвиг


# Выводы:
Цель данной задачи была доказать что качество линейной регрессии не меняется при преобразованных признаках и исходных.

$$
a = a_1
$$

Если для неизменённых данных действует обычная формула, то для преобразованных данных работает формула:

$$
a_1 = X_1w_1 + w_0
$$ 

Где:

$$
X_1 = XP
$$

Где:

$$
w_1 = (X_1^T X_1)^{-1} X_1^T y = ((XP)^TXP)^{-1}(XP)^Ty = 
$$
$$
(P^TX^TXP)^{-1}P^TX^Ty = P^{-1}(X^TX)^{-1}(P^T)^{-1}P^TX^Ty
$$

В данной фофрмуле произведение матриц $(P^T)^{-1}P^T$  представляют собой единичную матрицу $E$. А так как в ходе умножения матриц $AE$ или  $EA$  результатом будет матрица $A$, то уберем единичную матрицу из формулы.

Получим формулу:

$$
w_1 = P^{-1}(X^TX)^{-1}X^Ty = P^{-1}w
$$

Из этого следует что:

$$
a_1 = X_1w_1 = (XP)(P^{-1}w) = XPP^{-1}w
$$

Опять матрицы $PP^{-1}$ образовали матрицу $E$.
Далее:

$$
a = Xw = a_1
$$

Следовательно матрицы  $a_1$ и $a$ тождественны.

$$
a = a_1
$$

Применяя библиотечную функцию и собственную, к данным исходным и преобразованным получили результат в 42%. Т.к. результат идентичный, подтверждаем что при шифровании данных не влияет на качество линейной регрессии. 

# Библиотеки:
* pandas
* numpy
* sklearn
